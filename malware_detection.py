# -*- coding: utf-8 -*-
"""Malware_detection_all_classifiers(with proper docstrings).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ur0pkOM9G0AF_iqTF2ybeCuysjMvzA8q
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from  sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import confusion_matrix
from google.colab import files
from sklearn.metrics import ConfusionMatrixDisplay
from mlxtend.plotting import plot_confusion_matrix
import matplotlib as mlt
mlt.rcParams['figure.dpi']=300

"""**Mounting Drive**"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

"""**Reading and storing data for work**"""

malData= pd.read_csv("/content/drive/MyDrive/datasets/dataset_malwares.csv")

display(malData.shape)
malData.head()

"""**Analysing the data and dropping the columns**"""

malData.isnull().sum()

data1=malData.dropna(how='any',axis=0)
display(data1.shape)
malData.head()

data1["classification"].value_counts()

data1["classification"]=data1.classification.map({'benign':0,'malware':1})
data1.head()

malData["classification"].value_counts().plot(kind="pie",autopct="%1.1f%%")

plt.axis("equal")

plt.savefig('CLASSIFICATION.png')

#files.download('CLASSIFICATION.png')

feature= data1.drop(["hash","classification","vm_truncate_count","shared_vm","exec_vm","nvcsw","maj_flt","utime"],axis=1)
display(feature.shape)
feature.head()

target = data1["classification"]
target

"""**Scaling data for better results and less time complexity**"""

scaler = StandardScaler()

scaler.fit(feature.values)
scaler.mean_

scaled_feature = scaler.transform(feature.values)
scaled_feature

scaled_feature_df = pd.DataFrame(scaled_feature, columns = feature.columns)
scaled_feature_df.head()

"""**Splitting the datasets into training data and testing data**"""

x_train, x_test, y_train, y_test = train_test_split(scaled_feature_df, target , test_size=0.2, random_state=42)

"""**Logistic regression method**

"""

model=LogisticRegression()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('Logistic_cm.png')

files.download('Logistic_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**Naive bayes theorm**"""

model=GaussianNB()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('Naive_cm.png')

files.download('Naive_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**K-Neighbours**"""

model=KNeighborsClassifier()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('KNN_cm.png')

files.download('KNN_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**Gradient Boosting**"""

model=GradientBoostingClassifier()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('Gradient_cm.png')

files.download('Gradient_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**ADA Boosting**"""

model=AdaBoostClassifier()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('ADA_cm.png')

files.download('ADA_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**Support Vector Classifier**"""

model=SVC()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('SVC_cm.png')

files.download('SVC_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**Decision Tree**"""

model=DecisionTreeClassifier()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('DT_cm.png')

files.download('DT_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**Random Forest**"""

model=RandomForestClassifier()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

cm=confusion_matrix(y_test, y_pred)
cm

figure, ax = plot_confusion_matrix(conf_mat = cm,show_absolute = False,show_normed = True,colorbar = True)

plt.savefig('RF_cm.png')
files.download('RF_cm.png')

model.score(x_test,y_test)

mean_squared_error(y_test,y_pred)

r2_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

"""**GRAPH PLOTTING**"""

mycolor = ['red', 'saddlebrown', 'maroon', 'olive', 'orange', 'green', 'darkslategrey', 'royalblue']

names = ['Logistic Regression','ADA Boosting', 'Gradient Boosting', 'K-Neighbours','Support Vector Classifier','Decision Tree','Random Forest','Naive Bayes',]
values = [0.8931333333333333, 1.0, 1.0, 0.9996, 0.9935333333333334, 1.0, 1.0, 0.6464333333333333]
plt.bar(range(len(names)), values, align='edge', width=0.5,color=mycolor)
plt.xticks(range(len(values)), names, rotation='vertical')
plt.title('ACCURACY')

plt.show()

names = ['Logistic Regression','ADA Boosting', 'Gradient Boosting', 'K-Neighbours','Support Vector Classifier','Decision Tree','Random Forest','Naive Bayes',]
values = [0.10686666666666667, 0.0, 0.0, 0.0004, 0.006466666666666667, 0.0, 0.0, 0.35356666666666664]

plt.bar(range(len(names)), values, align='edge', width=0.5,color=mycolor)
plt.xticks(range(len(values)), names, rotation='vertical')
plt.title('Mean Squared Error')

plt.show()

names = ['Logistic Regression','ADA Boosting', 'Gradient Boosting', 'K-Neighbours','Support Vector Classifier','Decision Tree','Random Forest','Naive Bayes',]
values = [0.89, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.62]

plt.bar(range(len(names)), values, align='edge', width=0.5,color=mycolor)
plt.xticks(range(len(values)), names, rotation='vertical')
plt.title('F1 Score')

plt.show()

names = ['Logistic Regression','ADA Boosting', 'Gradient Boosting', 'K-Neighbours','Support Vector Classifier','Decision Tree','Random Forest','Naive Bayes',]
values = [0.5725315075679056, 1.0, 1.0, 0.998399993166193, 0.974133222853454, 1.0, 1.0, -0.4142727071825403]

plt.bar(range(len(names)), values, align='edge', width=0.5,color=mycolor)
plt.xticks(range(len(values)), names, rotation='vertical')
plt.title('R2 Score')

plt.show()

names = ['Logistic Regression','ADA Boosting', 'Gradient Boosting', 'K-Neighbours','Support Vector Classifier','Decision Tree','Random Forest','Naive Bayes',]
values = [0.89, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.71]

plt.bar(range(len(names)), values, align='edge', width=0.5,color=mycolor)
plt.xticks(range(len(values)), names, rotation='vertical')
plt.title('Precision')

plt.show()